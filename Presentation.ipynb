{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:07:40.480507Z",
     "start_time": "2019-03-19T11:07:40.465829Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Installation\n",
    "* Comparision with Pandas\n",
    "    * Common Operations\n",
    "* Data Sources for I/O\n",
    "    * Common Formats\n",
    "    * Local File System and HDFS\n",
    "    * Connecting with AWS, Cassandra & Postgres\n",
    "* Deployment Modes\n",
    "    * Client, Cluster, Local\n",
    "    * spark-submit\n",
    "* ETL & ML - <font color='red'>(Notebook Demo)</font>\n",
    "    * EDA and ETL Pipelines\n",
    "    * ML pipelines\n",
    "    * AWS EMR Cloud <font color='red'>(AWS Demo)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1 : Install Java, Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Install Scala and Java\n",
    "cd ~\n",
    "sudo apt install default-jre scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Step 2 : Download Spark and set SPARK_HOME in .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Download Spark\n",
    "wget https://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "tar xvf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "sudo mv spark-2.4.0-bin-hadoop2.7 /usr/local/spark\n",
    "\n",
    "## put these lines in bashrc\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "\n",
    "## refresh .bashrc file\n",
    "source .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Test it\n",
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Look - Compare with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark :\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('myfirst program')\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp = pd.read_csv(\"data/hotel_energy.csv\", \n",
    "                  header=0)\n",
    "\n",
    "# PySpark:\n",
    "dfs = spark.read.csv(\"data/hotel_energy.csv\", \n",
    "                     header=True, \n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Show DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas : \n",
    "dfp.head(15)\n",
    "\n",
    "# PySpark :\n",
    "dfs.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Column and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.columns\n",
    "dfp.dtypes\n",
    "\n",
    "# PySpark :\n",
    "dfs.columns\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change Column Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.columns = [\"a\", \"b\", \"c\"]\n",
    "\n",
    "dfp.rename(columns = {\"old1\":\"new1\",\n",
    "                      \"old2\":\"new2\"})\n",
    "\n",
    "# PySpark : \n",
    "dfs1 = df.toDF[\"a\", \"b\", \"c\"]\n",
    "\n",
    "dfs.withColumnRenamed(\"old1\", \"new1\")\\\n",
    "   .withColumnRenamed(\"old2\", \"new2\")\\\n",
    "   .withColumnRenamed(\"old3\", \"new2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.drop(\"hotel\", axis=1)\n",
    "\n",
    "# PySpark :\n",
    "dfs.drop(\"hotel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp[\"sales\"].astype('int')\n",
    "\n",
    "\n",
    "# PySpark :\n",
    "from pyspark.sql.functions import col\n",
    "df = dfs.withColumn(\"sales\", col(\"sales\").cast(\"int\"))\n",
    "\n",
    "BinaryType: binary\n",
    "BooleanType: boolean\n",
    "ByteType: tinyint\n",
    "DateType: date\n",
    "DecimalType: decimal(10,0)\n",
    "DoubleType: double\n",
    "FloatType: float\n",
    "IntegerType: int\n",
    "LongType: bigint\n",
    "ShortType: smallint\n",
    "StringType: string\n",
    "TimestampType: timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "dfs = dfs.withColumn(\"sales\", col(\"sales\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "df.groupby(['age', 'gender'])\\\n",
    "  .agg({'height':\"mean\", 'income':'min'})\n",
    "\n",
    "# Pyspark :    \n",
    "df.groupby(['age', 'gender'])\\\n",
    "  .agg({'height':\"mean\", 'income':'min'})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "import numpy as np\n",
    "df['log_sales'] = np.log(df[\"sales\"])\n",
    "\n",
    "# Pyspark:\n",
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('log_sales', F.log(df.sales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Difference between Pandas and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "# -\n",
    "\n",
    "# Pyspark:\n",
    "df.createOrReplaceTempView(\"df_VIEW\")\n",
    "ans_df = spark.sql(\"select * from df_VIEW where fruit == 'orange'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lazy Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** : When you run a cell, the contents are executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "# Cell Runs and data is loaded in memory\n",
    "df = pd.read_csv(\"cars.csv\") \n",
    "\n",
    "# executed and you get a new data frame right now\n",
    "df2 = df.filter(\"mileage > 30\") \n",
    "df3 = df2.select('carType').distinct()\n",
    "\n",
    "df3 # you get your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T07:28:25.756360Z",
     "start_time": "2019-03-21T07:28:25.747213Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Spark** : When you run an operation, they are not executed. Insted a recepie is created.\n",
    "\n",
    "This recepie is called a **DAG** (Directed Acyclic Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T07:59:27.461147Z",
     "start_time": "2019-03-21T07:59:27.350791Z"
    }
   },
   "outputs": [],
   "source": [
    "# PySpark\n",
    "# Cell Runs and data is loaded in memory\n",
    "df = spark.read.csv(\"cars.csv\") \n",
    "\n",
    "# Not executed, but a recepie is created\n",
    "df2 = df.filter(\"mileage > 30\")\n",
    "df3 = df2.select('carType').distinct()\n",
    "\n",
    "df3 # nothing happens - only a DAG is created\n",
    "\n",
    "df3.show() # execute the recepie - using an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dag1](./images/diff_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why Lazy Evaluation?\n",
    "Dealing with 10s/100s of GB of data, does not fit in RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "![Spark T/A](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pagecounts/trans_and_actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T07:48:14.312444Z",
     "start_time": "2019-03-21T07:48:14.306949Z"
    }
   },
   "source": [
    "* Transformations create a recepie\n",
    "\n",
    "* Actions execute the recepie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Immutability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T07:48:14.312444Z",
     "start_time": "2019-03-21T07:48:14.306949Z"
    }
   },
   "source": [
    "* <font color='red'> Every transformation/action gives a new dataframe </font>\n",
    "* Each new dataframe is immutable ( No inplace operations like Pandas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "df = read_csv(\"/file\")\n",
    "df.drop('age', inplace=True, axis=1) # df is changed\n",
    "\n",
    "# spark\n",
    "df = read.csv(\"/file\")\n",
    "df.drop('age') # df is not changed\n",
    "# returns a new df, with 'age' column dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark Data Sources](https://i2.wp.com/www.jenunderwood.com/wp-content/uploads/2016/10/SparkArchitecture-Databrickss.gif?resize=800%2C462&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common formats\n",
    "\n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* LibSVM\n",
    "* Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generic format\n",
    "spark.read.<format>(\"/path/to/file\")\n",
    "spark.write.<format>(\"/path/to/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV\n",
    "spark.read.csv(\"/path/to/file\")\n",
    "\n",
    "## Parquet\n",
    "spark.read.parquet(\"path/to/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If **HDFS** is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Local file\n",
    "spark.read.<format>.(\"file:///<full path here>\") # note the file:///  & path\n",
    "spark.write.<format>.(\"file:///<full path here>\") # note the file:/// & path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Formats  - Include Drivers and Jars\n",
    "* AWS\n",
    "* Cassandra\n",
    "* Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 - Prefer\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \\\n",
    "                                     <jar>,<jar> \\\n",
    "                                     pyspark-shell'\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('Demo 1')\\\n",
    "                    .master(\"local\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('postgres spark demo')\\\n",
    "                    .master(\"local\")\\\n",
    "                    .config(\"spark.jars.packages\", \"<jar>,<jar>\")\\\n",
    "                    .getOrCreate()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Driver & Jar Files\n",
    "\n",
    "Drivers from https://mvnrepository.com/\n",
    "\n",
    "Jar Format is **groupID:artifactID:version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Postgress Jar: \"org.postgresql:postgresql:42.2.1\"\n",
    "\n",
    "AWS Jar: \"org.apache.hadoop:hadoop-aws:2.7.1\"\n",
    "\n",
    "Cassnadra Jar: \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0\"\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.<format>(\"s3a://<bucket name>/<file name>\") # note the s3a://"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "          .option(\"spark.cassandra.connection.host\",\"<ip>\")\\\n",
    "          .option(\"spark.cassandra.connection.port\",\"<port>\")\\\n",
    "          .option(\"keyspace\",\"<keyspace name>\")\\\n",
    "          .option(\"table\",\"<table name>\")\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "          .option(\"spark.cassandra.connection.host\",\"localhost\")\\\n",
    "          .option(\"spark.cassandra.connection.port\",\"9042\")\\\n",
    "          .option(\"keyspace\",\"spark_demo_keyspace\")\\\n",
    "          .option(\"table\",\"fruits_prices_over_time\")\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Postgres - Mysql - Relational Databases\n",
    "spark.read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"<driver name>\")\\\n",
    "      .option(\"url\", \"jdbc:<dbtype>://<ip>:<port>/<dbname>\")\\\n",
    "      .option(\"dbtable\", \"<table>\")\\\n",
    "      .option(\"user\", \"<username>\")\\\n",
    "      .option(\"password\",\"<password>\")\\\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_demo_db\")\\\n",
    "      .option(\"dbtable\", \"fruits_prices_over_time\")\\\n",
    "      .option(\"user\", \"sahil\")\\\n",
    "      .option(\"password\",\"1234567890\")\\\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lets Create a Cluster - Standalone Cluster Manager\n",
    "\n",
    "* 1 Master and 1 Slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$SPARK_HOME/sbin/start-master.sh --host localhost\n",
    "\n",
    "\n",
    "$SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --memory 5G --cores 3\n",
    "# visit localhost:8080 for MASTER UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Deployment Modes\n",
    "\n",
    "* Client Mode\n",
    "\n",
    "* Cluster Mode\n",
    "\n",
    "* Local Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Client Mode\n",
    "* Driver runs on your laptop/master\n",
    "* Interactive Apps : Jupyter Notebook, Spark-Shell, Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Client Mode](./images/client_mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cluster Mode\n",
    "* Driver runs on worker node\n",
    "* spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Client Mode](./images/cluster_mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Local Mode\n",
    "* Driver and Worker runs on your laptop\n",
    "* Learning purpose only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Client Mode](./images/local_mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark-Submit\n",
    "\n",
    "Used to run scripts on cluster after prototyping\n",
    "\n",
    "* Remove master from script and PySpark Submit Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \\\n",
    "                                     <jar>,<jar> \\\n",
    "                                     pyspark-shell'\n",
    "spark = SparkSession.builder.appName('myfirst program')\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "# script\n",
    "import os\n",
    "spark = SparkSession.builder.appName('myfirst program')\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Generic Spark Submit\n",
    "spark-submit --packages <font color='green'>jar,jar</font> --master <font color='green'>ip</font>  script_name.py\n",
    "\n",
    "#### Local Mode\n",
    "spark-submit --packages jar,jar --master <font color='red'>local[6]</font> script_name\n",
    "\n",
    "#### For Standalone Cluster Manager (No Cluster Mode):\n",
    "spark-submit --packages jar,jar --master <font color='red'>spark://ip:7077</font> script_name\n",
    "\n",
    "#### For YARN Cluster Manager:\n",
    "spark-submit     --packages jar,jar --master <font color='red'>yarn</font> --deploy-mode <font color='red'>cluster/client</font> script_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# local\n",
    "spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.1 \\\n",
    "--master local[6] \\\n",
    "ETL_aws.py\n",
    "\n",
    "# standalone\n",
    "spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.1 \\\n",
    "--master spark://localhost:7077 \\\n",
    "ETL_aws.py\n",
    "                \n",
    "# yarn\n",
    "spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.1 \\\n",
    "--master yarn \\\n",
    "ETL_aws.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NoteBook Demo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
