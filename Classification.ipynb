{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:12.267147Z",
     "start_time": "2019-03-21T17:55:12.251677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:24.534418Z",
     "start_time": "2019-03-21T17:55:14.841997Z"
    }
   },
   "outputs": [],
   "source": [
    "# PySpark :\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \\\n",
    "                                    org.postgresql:postgresql:42.1.1,org.apache.hadoop:hadoop-aws:2.7.1,com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 \\\n",
    "                                    --executor-memory 4G \\\n",
    "                                    pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder.appName('Classification with Spark')\\\n",
    "                            .master(\"spark://localhost:7077\")\\\n",
    "                            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:24.542070Z",
     "start_time": "2019-03-21T17:55:24.535895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.80:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Classification with Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5b5b1b1c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:55.843732Z",
     "start_time": "2019-03-21T17:55:52.554101Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"./data/fireservice_data_for_ML.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:56.583781Z",
     "start_time": "2019-03-21T17:55:56.494568Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(\"features\", \"FinalPriority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:55:59.489111Z",
     "start_time": "2019-03-21T17:55:57.420325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|FinalPriority|\n",
      "+--------------------+-------------+\n",
      "|(198,[1,5,27,48,8...|            3|\n",
      "|(198,[1,5,27,48,8...|            3|\n",
      "|(198,[0,5,27,48,8...|            3|\n",
      "|(198,[0,5,27,55,8...|            3|\n",
      "|(198,[1,5,22,55,8...|            3|\n",
      "|(198,[0,5,27,55,8...|            3|\n",
      "|(198,[0,7,22,33,8...|            3|\n",
      "|(198,[1,7,27,33,8...|            3|\n",
      "|(198,[0,7,27,33,8...|            3|\n",
      "|(198,[1,10,27,48,...|            3|\n",
      "|(198,[1,10,27,48,...|            3|\n",
      "|(198,[1,10,27,48,...|            3|\n",
      "|(198,[0,10,27,48,...|            3|\n",
      "|(198,[0,11,27,48,...|            3|\n",
      "|(198,[0,11,27,48,...|            3|\n",
      "|(198,[0,9,27,33,8...|            3|\n",
      "|(198,[1,9,27,33,8...|            3|\n",
      "|(198,[0,9,27,33,8...|            3|\n",
      "|(198,[1,5,20,48,8...|            3|\n",
      "|(198,[1,5,27,48,8...|            3|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:59:58.310230Z",
     "start_time": "2019-03-21T17:59:55.663715Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='FinalPriority',\n",
    "                        outputCol = 'label',\n",
    "                        handleInvalid=\"keep\",        # last index will be for invalid values not encountered before\n",
    "                        stringOrderType='alphabetAsc')\n",
    "\n",
    "new_df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:03.322240Z",
     "start_time": "2019-03-21T18:00:02.788249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----+\n",
      "|            features|FinalPriority|label|\n",
      "+--------------------+-------------+-----+\n",
      "|(198,[1,5,27,48,8...|            3|  1.0|\n",
      "|(198,[1,5,27,48,8...|            3|  1.0|\n",
      "|(198,[0,5,27,48,8...|            3|  1.0|\n",
      "|(198,[0,5,27,55,8...|            3|  1.0|\n",
      "|(198,[1,5,22,55,8...|            3|  1.0|\n",
      "+--------------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:17.505344Z",
     "start_time": "2019-03-21T18:00:17.492919Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = new_df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:18.020235Z",
     "start_time": "2019-03-21T18:00:18.008759Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.repartition(3)\n",
    "test = test.repartition(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:19.639223Z",
     "start_time": "2019-03-21T18:00:19.628674Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import accuracy_score\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"FinalPriority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:20.038976Z",
     "start_time": "2019-03-21T18:00:20.029005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='BinaryClassificationEvaluator_09ed4aecb1ec', name='labelCol', doc='label column name.'): 'FinalPriority',\n",
       " Param(parent='BinaryClassificationEvaluator_09ed4aecb1ec', name='metricName', doc='metric name in evaluation (areaUnderROC|areaUnderPR)'): 'areaUnderROC',\n",
       " Param(parent='BinaryClassificationEvaluator_09ed4aecb1ec', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:17:53.330813Z",
     "start_time": "2019-03-21T18:17:53.310089Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', standardization=False, maxIter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:26:44.626750Z",
     "start_time": "2019-03-21T18:26:21.205187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:16:44.181968Z",
     "start_time": "2019-03-21T18:16:44.120780Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:17:04.959808Z",
     "start_time": "2019-03-21T18:16:45.175728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:17:27.064213Z",
     "start_time": "2019-03-21T18:17:04.961325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc : 1.0\n"
     ]
    }
   ],
   "source": [
    "# areaUnderROC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Auc : {}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:12:37.553231Z",
     "start_time": "2019-03-21T18:12:37.543612Z"
    }
   },
   "outputs": [],
   "source": [
    "### Classification Model\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\",\n",
    "                            labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:14:52.184221Z",
     "start_time": "2019-03-21T18:12:40.660647Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_model = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:15:17.458772Z",
     "start_time": "2019-03-21T18:14:55.355297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc : 1.0\n"
     ]
    }
   ],
   "source": [
    "# prdict\n",
    "predictions = dt_model.transform(test)\n",
    "\n",
    "# areaUnderROC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Auc : {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:15:33.226097Z",
     "start_time": "2019-03-21T18:15:33.203692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DecisionTreeClassificationModel (uid=DecisionTreeClassifier_209fd8ebd4d3) of '\n",
      " 'depth 5 with 31 nodes\\n'\n",
      " '  If (feature 189 in {0.0})\\n'\n",
      " '   If (feature 190 in {0.0})\\n'\n",
      " '    If (feature 48 in {0.0})\\n'\n",
      " '     If (feature 57 in {0.0})\\n'\n",
      " '      If (feature 31 in {0.0})\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 31 not in {0.0})\\n'\n",
      " '       Predict: 0.0\\n'\n",
      " '     Else (feature 57 not in {0.0})\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '    Else (feature 48 not in {0.0})\\n'\n",
      " '     If (feature 27 in {0.0})\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '     Else (feature 27 not in {0.0})\\n'\n",
      " '      If (feature 185 in {0.0})\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 185 not in {0.0})\\n'\n",
      " '       Predict: 0.0\\n'\n",
      " '   Else (feature 190 not in {0.0})\\n'\n",
      " '    If (feature 20 in {0.0})\\n'\n",
      " '     If (feature 80 in {0.0})\\n'\n",
      " '      If (feature 48 in {0.0})\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 48 not in {0.0})\\n'\n",
      " '       Predict: 0.0\\n'\n",
      " '     Else (feature 80 not in {0.0})\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '    Else (feature 20 not in {0.0})\\n'\n",
      " '     Predict: 1.0\\n'\n",
      " '  Else (feature 189 not in {0.0})\\n'\n",
      " '   If (feature 80 in {0.0})\\n'\n",
      " '    If (feature 55 in {0.0})\\n'\n",
      " '     If (feature 20 in {0.0})\\n'\n",
      " '      Predict: 0.0\\n'\n",
      " '     Else (feature 20 not in {0.0})\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '    Else (feature 55 not in {0.0})\\n'\n",
      " '     If (feature 19 in {0.0})\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '     Else (feature 19 not in {0.0})\\n'\n",
      " '      If (feature 164 in {0.0})\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 164 not in {0.0})\\n'\n",
      " '       Predict: 0.0\\n'\n",
      " '   Else (feature 80 not in {0.0})\\n'\n",
      " '    Predict: 1.0\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:18:14.130828Z",
     "start_time": "2019-03-21T18:18:14.115137Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.1, 0.9])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:22:18.239851Z",
     "start_time": "2019-03-21T18:18:15.281400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create k-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:22:46.603876Z",
     "start_time": "2019-03-21T18:22:24.385852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc : 1.0\n"
     ]
    }
   ],
   "source": [
    "# prdict\n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "# areaUnderROC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Auc : {}\".format(auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:23:08.582328Z",
     "start_time": "2019-03-21T18:23:08.576157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_f75fbd9a598c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.1,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='maxIter', doc='maximum number of iterations (>= 0)'): 2,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='regParam', doc='regularization parameter (>= 0)'): 0.01,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='standardization', doc='whether to standardize the training features before fitting the model'): False,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_f75fbd9a598c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.extractParamMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
